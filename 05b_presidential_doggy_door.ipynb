{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have trained accurate models on large datasets, and also downloaded a pre-trained model that we used with no training necessary. But what if we cannot find a pre-trained model that does exactly what you need, and what if we do not have a sufficiently large dataset to train a model from scratch? In this case, there is a very helpful technique we can use called [transfer learning](https://blogs.nvidia.com/blog/2019/02/07/what-is-transfer-learning/).\n",
    "\n",
    "With transfer learning, we take a pre-trained model and retrain it on a task that has some overlap with the original training task. A good analogy for this is an artist who is skilled in one medium, such as painting, who wants to learn to practice in another medium, such as charcoal drawing. We can imagine that the skills they learned while painting would be very valuable in learning how to draw with charcoal. \n",
    "\n",
    "As an example in deep learning, say we have a pre-trained model that is very good at recognizing different types of cars, and we want to train a model to recognize types of motorcycles. A lot of the learnings of the car model would likely be very useful, for instance the ability to recognize headlights and wheels. \n",
    "\n",
    "Transfer learning is especially powerful when we do not have a large and varied dataset. In this case, a model trained from scratch would likely memorize the training data quickly, but not be able to generalize well to new data. With transfer learning, you can increase your chances of training an accurate and robust model on a small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b.1 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Prepare a pretrained model for transfer learning\n",
    "* Perform transfer learning with your own small dataset on a pretrained model\n",
    "* Further fine tune the model for even better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b.2 A Personalized Doggy Door"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our last exercise, we used a pre-trained [ImageNet](http://www.image-net.org/) model to let in all dogs, but keep out other animals. In this exercise, we would like to create a doggy door that only lets in a particular dog. In this case, we will make an automatic doggy door for a dog named Bo, the United States First Dog between 2009 and 2017. There are more pictures of Bo in the `data/presidential_doggy_door` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/presidential_doggy_door/train/bo/bo_10.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is that the pre-trained model was not trained to recognize this specific dog, and, we only have 30 pictures of Bo. If we tried to train a model from scratch using those 30 pictures we would experience overfitting and poor generalization. However, if we start with a pre-trained model that is adept at detecting dogs, we can leverage that learning to gain a generalized understanding of Bo using our smaller dataset. We can use transfer learning to solve this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.2.1 Downloading the Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [ImageNet torchvision.models](https://pytorch.org/vision/stable/models.html) are often good choices for computer vision transfer learning, as they have learned to classify various different types of images. In doing this, they have learned to detect many different types of [features](https://developers.google.com/machine-learning/glossary#) that could be valuable in image recognition. Because ImageNet models have learned to detect animals, including dogs, it is especially well suited for this transfer learning task of detecting Bo.\n",
    "\n",
    "Let us start by downloading the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# load the VGG16 network *pre-trained* on the ImageNet dataset\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are downloading, there is going to be an important difference. The last layer of an ImageNet model is a [dense layer](https://developers.google.com/machine-learning/glossary#dense-layer) of 1000 units, representing the 1000 possible classes in the dataset. In our case, we want it to make a different classification: is this Bo or not? We will add new layers to specifically recognize Bo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.2.2 Freezing the Base Model\n",
    "Before we add our new layers onto the [pre-trained model](https://developers.google.com/machine-learning/glossary#pre-trained-model), let's take an important step: freezing the model's pre-trained layers. This means that when we train, we will not update the base layers from the pre-trained model. Instead we will only update the new layers that we add on the end for our new classification. We freeze the initial layers because we want to retain the learning achieved from training on the ImageNet dataset. If they were unfrozen at this stage, we would likely destroy this valuable information. There will be an option to unfreeze and train these layers later, in a process called fine-tuning.\n",
    "\n",
    "Freezing the base layers is as simple as setting [requires_grad_](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html) on the model to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Frozen\n"
     ]
    }
   ],
   "source": [
    "vgg_model.requires_grad_(False)\n",
    "print(\"VGG16 Frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.2.3 Adding New Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the new trainable layers to the pre-trained model. They will take the features from the pre-trained layers and turn them into predictions on the new dataset. We will add two layers to the model. In a previous lesson, we created our own [custom module](https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html). A transfer learning module works in the exact same way. We can use is a layer in a [Sequential Model](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n",
    "\n",
    "Then, we'll add a `Linear` layer connecting all `1000` of VGG16's outputs to `1` neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=1000, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_CLASSES = 1\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model,\n",
    "    nn.Linear(1000, N_CLASSES)\n",
    ")\n",
    "\n",
    "my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to verify that the VGG layers are frozen, we can loop through the model [parameters](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 False\n",
      "3 False\n",
      "4 False\n",
      "5 False\n",
      "6 False\n",
      "7 False\n",
      "8 False\n",
      "9 False\n",
      "10 False\n",
      "11 False\n",
      "12 False\n",
      "13 False\n",
      "14 False\n",
      "15 False\n",
      "16 False\n",
      "17 False\n",
      "18 False\n",
      "19 False\n",
      "20 False\n",
      "21 False\n",
      "22 False\n",
      "23 False\n",
      "24 False\n",
      "25 False\n",
      "26 False\n",
      "27 False\n",
      "28 False\n",
      "29 False\n",
      "30 False\n",
      "31 False\n",
      "32 True\n",
      "33 True\n"
     ]
    }
   ],
   "source": [
    "for idx, param in enumerate(my_model.parameters()):\n",
    "    print(idx, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we did want to make the VGG layers trainable, we could take `vgg_model` and set `requires_grad_` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Unfrozen\n",
      "0 True\n",
      "1 True\n",
      "2 True\n",
      "3 True\n",
      "4 True\n",
      "5 True\n",
      "6 True\n",
      "7 True\n",
      "8 True\n",
      "9 True\n",
      "10 True\n",
      "11 True\n",
      "12 True\n",
      "13 True\n",
      "14 True\n",
      "15 True\n",
      "16 True\n",
      "17 True\n",
      "18 True\n",
      "19 True\n",
      "20 True\n",
      "21 True\n",
      "22 True\n",
      "23 True\n",
      "24 True\n",
      "25 True\n",
      "26 True\n",
      "27 True\n",
      "28 True\n",
      "29 True\n",
      "30 True\n",
      "31 True\n",
      "32 True\n",
      "33 True\n"
     ]
    }
   ],
   "source": [
    "vgg_model.requires_grad_(True)\n",
    "print(\"VGG16 Unfrozen\")\n",
    "for idx, param in enumerate(my_model.parameters()):\n",
    "    print(idx, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for now, we'd only like to train our new layers, so we will turn training off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 Frozen\n"
     ]
    }
   ],
   "source": [
    "vgg_model.requires_grad_(False)\n",
    "print(\"VGG16 Frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Compiling the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our previous exercises, we need to compile the model with loss and metrics options. We have to make some different choices here. In previous cases we had many categories in our classification problem. As a result, we picked categorical crossentropy for the calculation of our loss. In this case we only have a binary classification problem (Bo or not Bo), and so we will use [binary crossentropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html). Further detail about the differences between the two can found [here](https://gombru.github.io/2018/05/23/cross_entropy_loss/). We will also use binary accuracy instead of traditional accuracy.\n",
    "\n",
    "By setting `from_logits=True` we inform the [loss function](https://gombru.github.io/2018/05/23/cross_entropy_loss/) that the output values are not normalized (e.g. with softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(my_model.parameters())\n",
    "my_model = my_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b.3 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in the previous lessons, we'll create a custom [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to read in picutes of Bo (and not Bo). First, we'll grab the list of preprocessing transforms from the VGG `weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trans = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.3.1 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than read from a DataFrame like in previous lessons, we will read image files directly and infer the `label` based on the filepath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LABELS = [\"bo\", \"not_bo\"] \n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.jpg', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = Image.open(path)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device).float())\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.3.2 The DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our custom Dataset class, let's create our [DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32\n",
    "\n",
    "train_path = \"data/presidential_doggy_door/train/\"\n",
    "train_data = MyDataset(train_path)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_path = \"data/presidential_doggy_door/valid/\"\n",
    "valid_data = MyDataset(valid_path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b.3.3 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply some data augmentation so the model can have a better chance at recognizing Bo. This time, we have color images, so we can use [ColorJitter](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_illustrations.html#colorjitter) to full effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.8, 1), ratio=(1, 1)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=.2, contrast=.2, saturation=.2, hue=.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b.4 The Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use most of the same training loop as before but with a few slight differences. First, our `get_batch_accuracy` function will be different because of using [Binary Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) as our loss function. We could run the output through the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function, but we can be more efficient by being mathematically observant.\n",
    "\n",
    "When our model `output` is greater than `0`, running it through the sigmoid function would be closer to `1`. When the model `output` is less than `0`, running it through the sigmoid function would be closer to `0`. Therefore, we only need to check if the model output is greater than ([gt](https://pytorch.org/docs/stable/generated/torch.gt.html)) `0` to see which class our prediction leans towards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(output, y, N):\n",
    "    zero_tensor = torch.tensor([0]).to(device)\n",
    "    pred = torch.gt(output, zero_tensor)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also section to print the last set of gradients to show that only our newly added layers are learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, check_grad=False):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        output = torch.squeeze(model(random_trans(x)))\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_function(output, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += get_batch_accuracy(output, y, train_N)\n",
    "    if check_grad:\n",
    "        print('Last Gradient:')\n",
    "        for param in model.parameters():\n",
    "            print(param.grad)\n",
    "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the below to see a sample of the model's gradients. Because VGG16 ends in 1000 neurons, there are 1000 weights connected to the single neuron in the next layer. Many numbers will be printed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Gradient:\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "tensor([[-1.1008e-01, -1.9255e-01, -1.0102e-01, -2.3838e-01, -3.0941e-01,\n",
      "         -4.0953e-01, -3.3514e-01,  4.2310e-02,  2.2706e-02,  9.4034e-02,\n",
      "          1.4922e-01,  8.0744e-02,  4.7896e-02,  9.0361e-02, -1.0209e-01,\n",
      "          9.9402e-02,  1.4359e-02, -2.0647e-02, -1.4949e-02,  1.1043e-01,\n",
      "          1.1244e-01, -9.5454e-02, -4.2020e-02, -1.0321e-01,  1.2821e-01,\n",
      "          5.6172e-02, -4.2772e-02,  8.1629e-03, -6.5306e-03, -1.5706e-01,\n",
      "          2.4486e-02, -8.6581e-03,  9.4459e-03, -2.3120e-01, -2.4269e-01,\n",
      "         -4.7852e-03, -1.4998e-01,  1.0347e-01, -5.8147e-02, -1.7101e-02,\n",
      "         -5.8386e-03,  1.0025e-01,  4.7577e-02, -5.8854e-02,  1.5480e-01,\n",
      "         -5.8783e-03, -4.2527e-02, -1.2296e-01,  9.3055e-02,  1.6888e-02,\n",
      "         -1.1454e-01, -2.1754e-01,  1.2849e-01,  1.0077e-01,  1.2362e-01,\n",
      "         -1.5062e-02,  1.6246e-01,  2.6002e-01, -9.2027e-02, -1.5464e-02,\n",
      "          7.5910e-02,  1.4856e-01,  1.6941e-01,  8.1734e-02, -6.6133e-02,\n",
      "         -1.3931e-01,  1.0697e-01,  1.2412e-01,  1.2892e-01,  3.0112e-02,\n",
      "          1.4873e-01, -6.6432e-02,  6.6375e-02,  9.7091e-03,  8.2284e-02,\n",
      "          8.2193e-02, -1.5631e-02,  5.2911e-02, -5.3533e-02,  3.7154e-02,\n",
      "         -1.3933e-01,  1.5106e-01,  1.0091e-03,  1.0579e-01, -9.6693e-04,\n",
      "          2.1529e-02,  1.1681e-01, -9.2584e-03, -1.0037e-01, -3.5983e-02,\n",
      "          6.5538e-02, -1.2289e-01,  5.4467e-02, -9.6013e-03, -1.0985e-01,\n",
      "          5.0121e-02, -1.3759e-01,  1.6974e-01,  1.3172e-01, -1.2995e-02,\n",
      "         -7.9840e-02, -1.0878e-01,  9.8595e-02, -2.8364e-01, -4.1235e-03,\n",
      "          3.1504e-03, -2.3986e-02, -2.7457e-01, -1.1530e-01, -1.3204e-01,\n",
      "         -8.1161e-02, -1.4113e-01, -1.2605e-01, -6.8951e-02, -7.0806e-03,\n",
      "         -1.3937e-01, -2.9764e-02, -8.6189e-02, -3.3666e-02,  2.1686e-02,\n",
      "          8.6913e-02,  5.4390e-02,  4.8455e-02,  6.0197e-03, -5.4185e-02,\n",
      "         -2.9794e-02, -1.3897e-01,  9.8524e-02, -3.4323e-02, -8.3611e-02,\n",
      "          6.3956e-02, -6.5412e-02, -3.7231e-02,  9.6121e-02,  5.2824e-02,\n",
      "         -4.1310e-02, -4.6908e-03, -1.5336e-02,  1.6136e-01,  2.6541e-01,\n",
      "          2.2750e-01,  2.0664e-01,  2.1093e-01,  9.9129e-02,  9.6630e-02,\n",
      "         -4.5889e-02,  2.8358e-02, -2.1081e-02, -1.4186e-01, -2.3312e-01,\n",
      "         -2.1276e-01, -1.2431e-01,  8.2509e-02, -5.4980e-02,  7.4380e-02,\n",
      "         -3.7304e-03,  8.4150e-02, -1.9037e-03,  3.9330e-02,  4.3005e-02,\n",
      "         -2.4454e-02, -2.0236e-02,  4.3241e-02, -4.9355e-02,  3.7072e-02,\n",
      "          1.8437e-02,  1.0274e-01,  1.4284e-01,  2.5104e-02,  9.9520e-02,\n",
      "         -4.6053e-03,  1.4592e-02,  1.1023e-02, -5.8443e-03,  3.9809e-02,\n",
      "          1.1719e-01,  6.0003e-02, -4.0754e-02, -4.8719e-02, -1.1900e-01,\n",
      "         -2.8836e-02, -4.9025e-03, -8.3524e-03, -1.5287e-01,  7.7864e-03,\n",
      "         -1.4679e-02, -5.7344e-02, -2.5834e-02, -3.1061e-02, -6.3030e-02,\n",
      "         -1.2174e-02, -7.2931e-02, -2.6583e-02,  8.8786e-03,  5.4529e-02,\n",
      "         -1.3312e-01, -9.8976e-02, -1.5283e-01, -1.5824e-02, -2.2869e-01,\n",
      "          4.4153e-02,  1.1913e-01,  2.3269e-02, -1.3487e-01, -2.4577e-05,\n",
      "         -1.5645e-01, -1.1851e-01,  1.8097e-04, -1.1750e-01, -7.6397e-02,\n",
      "          4.3802e-02, -3.0036e-02,  7.2376e-02, -1.9457e-03, -5.3621e-02,\n",
      "          5.6541e-02,  7.2256e-02, -3.4039e-04,  9.9239e-03, -1.3002e-02,\n",
      "         -1.4711e-02, -6.2577e-02, -2.4589e-02, -2.0381e-01, -2.1696e-01,\n",
      "         -2.1950e-02, -9.9506e-02, -8.4277e-02,  7.8565e-02, -1.2672e-02,\n",
      "          3.0836e-02,  2.6440e-02, -2.4113e-03, -1.1404e-01, -1.6942e-02,\n",
      "         -1.0699e-02, -5.8452e-03, -1.3995e-01,  2.0707e-02,  1.4470e-03,\n",
      "          2.7135e-04,  2.5650e-03, -8.5331e-03,  2.9600e-02,  6.0742e-02,\n",
      "         -5.9626e-02, -1.0737e-02,  2.6525e-02,  9.8243e-02, -3.5811e-02,\n",
      "         -4.8059e-02,  4.6440e-02, -1.1028e-01,  1.0479e-03, -1.2488e-01,\n",
      "          5.4823e-02, -1.7270e-01,  2.7810e-02,  6.3830e-03, -1.0089e-01,\n",
      "         -5.3869e-02,  3.6975e-02, -3.1946e-02, -1.0766e-02, -6.4893e-02,\n",
      "         -1.6019e-01, -5.5038e-02, -1.1690e-01, -2.5966e-02, -2.4808e-02,\n",
      "         -2.1417e-02, -2.3983e-02,  4.4226e-02, -4.9104e-02,  2.7512e-02,\n",
      "          1.4357e-01,  8.3330e-02, -5.2289e-02,  4.8974e-02, -2.7982e-02,\n",
      "         -3.5999e-02, -5.6648e-02, -1.1473e-02, -2.0865e-02, -8.2774e-02,\n",
      "         -5.2269e-02, -1.3086e-01, -5.1465e-03,  2.5739e-02,  1.5803e-01,\n",
      "         -2.2391e-02,  5.0548e-02,  4.6042e-02,  6.3596e-02, -1.2005e-01,\n",
      "         -2.6357e-01, -1.6184e-01, -1.3900e-01, -2.1016e-02,  2.2746e-02,\n",
      "          5.6139e-02,  2.6919e-02,  1.1170e-02,  1.6505e-02, -1.0361e-02,\n",
      "          4.5383e-02, -5.1383e-02,  2.6402e-02,  3.7395e-03, -5.7271e-02,\n",
      "         -1.0712e-01,  1.0366e-01,  5.1780e-02,  1.3819e-02, -5.9761e-02,\n",
      "          2.4548e-03,  5.5450e-02,  8.2748e-02,  2.6854e-02, -5.8934e-03,\n",
      "          2.3847e-02, -3.5673e-02, -3.2780e-02,  5.7126e-03, -2.6747e-02,\n",
      "         -6.2495e-02, -1.9278e-02, -1.0497e-01, -6.0398e-02, -9.5593e-02,\n",
      "         -2.9361e-02, -1.5285e-02,  2.2423e-02,  7.4877e-04, -1.0802e-01,\n",
      "          3.2847e-02,  1.2447e-01, -1.3738e-01,  2.9857e-02,  8.8987e-02,\n",
      "          4.6135e-02, -7.8290e-02, -1.6937e-02,  1.5011e-02, -3.8179e-02,\n",
      "         -9.7768e-02, -8.8710e-02, -7.7023e-02,  6.1183e-02, -1.7205e-02,\n",
      "         -4.8298e-02,  4.5906e-02,  4.6805e-05,  3.1765e-03, -7.9646e-02,\n",
      "          8.1698e-03, -1.7671e-01, -8.6890e-02, -1.3806e-01, -4.9987e-02,\n",
      "         -1.0107e-01, -1.2766e-01, -1.1456e-01, -3.0683e-02, -1.7970e-01,\n",
      "         -1.4173e-01, -1.3284e-01, -1.6747e-01, -7.5675e-02, -1.7391e-01,\n",
      "          2.8102e-04, -1.5282e-02, -6.0357e-02, -9.0640e-03, -1.3032e-01,\n",
      "         -7.4869e-02, -1.3786e-01, -4.5398e-02, -2.1116e-01, -2.4606e-01,\n",
      "         -1.6862e-01, -1.6479e-01, -2.8326e-02, -5.2269e-03, -1.7307e-01,\n",
      "         -8.0513e-02, -1.1842e-01, -9.4011e-02, -5.4919e-02, -9.1915e-02,\n",
      "         -1.2766e-01, -1.3269e-01, -2.2506e-01, -9.1568e-02, -1.3684e-01,\n",
      "         -1.5031e-01, -1.6239e-01, -8.0262e-02,  1.1036e-01, -2.2594e-01,\n",
      "         -4.7096e-02,  9.5389e-03,  6.4539e-03,  1.5992e-01,  6.4813e-02,\n",
      "         -4.2468e-02,  7.5878e-02,  3.1749e-02,  9.8152e-02, -1.2456e-01,\n",
      "          6.7179e-02, -1.1069e-01, -7.6470e-02, -1.6407e-01, -6.8323e-02,\n",
      "          1.4125e-01,  1.8278e-02, -1.3733e-01,  1.9092e-01, -1.6129e-02,\n",
      "          3.1563e-03,  1.1671e-01,  7.5140e-02,  8.7183e-02,  1.0338e-01,\n",
      "          8.0188e-02,  5.1419e-02,  5.4134e-02, -1.8289e-02,  7.8142e-02,\n",
      "          7.6871e-02, -4.9097e-03, -3.0231e-02, -1.5182e-01,  4.7669e-02,\n",
      "         -5.0704e-02,  1.4289e-01,  8.4603e-02, -8.3048e-02,  1.8645e-02,\n",
      "          3.9750e-02,  8.3962e-02,  4.1948e-02, -1.2699e-01,  1.2357e-01,\n",
      "         -8.3593e-02, -1.1540e-01, -9.8173e-02, -1.2986e-01,  5.9533e-02,\n",
      "          4.6134e-02, -4.8333e-03, -9.1391e-02,  1.3377e-01,  1.3856e-01,\n",
      "          5.7574e-02, -9.0845e-02, -1.5185e-01,  5.8198e-02, -1.6316e-01,\n",
      "          1.2883e-01, -4.2597e-02, -6.4983e-02, -1.4781e-01,  8.3990e-02,\n",
      "         -1.4680e-01,  1.1634e-01,  2.2869e-01,  5.3831e-02, -6.4219e-02,\n",
      "         -1.1193e-01,  8.2351e-02,  2.3077e-02, -6.5083e-04, -7.7195e-02,\n",
      "         -3.1329e-02,  1.2071e-01,  1.5571e-01, -1.1339e-02,  1.3642e-01,\n",
      "         -6.5487e-03, -6.2253e-02,  9.9878e-02,  4.4726e-02,  6.4528e-02,\n",
      "          3.4115e-02,  1.6445e-02, -9.7515e-02,  2.7444e-02,  1.0641e-01,\n",
      "          2.0949e-02, -8.2933e-02,  6.8098e-02,  7.5970e-02, -1.6470e-02,\n",
      "          1.1057e-01, -6.2091e-02,  8.4823e-02,  1.2820e-01, -1.9130e-02,\n",
      "          4.4157e-02, -1.9288e-01,  4.7572e-02,  1.9348e-01, -3.9893e-03,\n",
      "          4.6554e-02,  9.7174e-02,  1.1800e-01,  6.6126e-02,  2.4026e-01,\n",
      "          1.1769e-01,  1.4633e-01,  4.8192e-02, -4.1619e-03,  3.6429e-02,\n",
      "         -1.0423e-01, -4.6191e-02,  5.2300e-02, -1.0495e-01,  1.0900e-01,\n",
      "          2.0600e-02,  1.3568e-04,  6.9586e-02,  2.1571e-02, -3.5478e-02,\n",
      "          1.0428e-01,  2.0864e-03, -6.6576e-02,  1.0316e-01, -1.0668e-01,\n",
      "         -7.4484e-02, -1.1700e-02,  5.8155e-02,  1.4598e-01,  3.6658e-02,\n",
      "          1.9254e-01,  2.4086e-01,  5.7551e-02,  5.5886e-02,  2.2297e-02,\n",
      "          9.2997e-02,  7.1776e-02,  2.3710e-02,  2.2763e-02,  8.3310e-02,\n",
      "          5.5757e-02, -1.0292e-01,  2.9072e-01,  7.5162e-02, -1.7413e-01,\n",
      "          2.2246e-01,  6.0613e-02, -1.5318e-01,  2.3761e-02,  1.4570e-01,\n",
      "          1.5050e-01, -4.2831e-03, -4.8346e-02, -5.8873e-02,  6.2306e-02,\n",
      "          4.5824e-02,  1.3721e-01,  3.2556e-02,  2.0049e-01,  1.4527e-01,\n",
      "          1.6109e-01, -1.6813e-02,  5.2754e-02, -4.8230e-02, -2.4764e-03,\n",
      "         -1.6886e-01,  2.0789e-01,  2.0592e-02,  1.6829e-01, -5.2858e-02,\n",
      "         -3.0898e-02,  2.3088e-02, -8.1773e-02, -4.1260e-02,  2.6942e-02,\n",
      "          2.9915e-03,  2.0038e-01,  1.9624e-01,  1.2508e-02, -1.2546e-01,\n",
      "         -2.8724e-02,  4.8808e-02,  5.5801e-02, -2.5751e-02, -7.2233e-02,\n",
      "          2.8112e-02, -7.4785e-02,  1.1604e-01,  3.2756e-02,  9.2663e-02,\n",
      "          9.6277e-02, -4.5027e-02,  3.7578e-02, -5.3950e-02,  3.3061e-02,\n",
      "         -1.1781e-01, -7.4392e-02, -8.4401e-03,  1.0637e-01,  8.3857e-02,\n",
      "         -9.9149e-02, -4.5206e-02, -1.0196e-02,  2.8086e-03,  6.5628e-02,\n",
      "         -2.3915e-01, -2.2575e-01,  1.3878e-01, -1.4470e-02, -5.7341e-02,\n",
      "         -5.4883e-02,  5.8911e-02, -6.5910e-02,  1.0747e-02, -6.6019e-02,\n",
      "         -2.4586e-01,  5.7421e-02,  6.1263e-02,  7.2394e-02,  1.9807e-01,\n",
      "          1.6547e-01,  6.4811e-03,  4.3730e-02,  1.6821e-01,  8.0397e-02,\n",
      "          3.1909e-02,  3.1297e-02, -2.8269e-02, -4.0272e-02,  2.1711e-01,\n",
      "         -2.2196e-02, -1.2076e-01,  5.8778e-02, -1.1073e-01, -1.1754e-01,\n",
      "          2.1908e-01, -3.9883e-02,  3.6555e-02, -1.0475e-01,  1.8554e-02,\n",
      "          4.6670e-02, -8.4192e-03,  9.2536e-02,  1.4584e-01, -3.5259e-02,\n",
      "         -6.4714e-02,  3.2963e-02, -7.3531e-02,  5.2668e-02,  6.2282e-02,\n",
      "         -7.9370e-02,  2.2067e-02,  5.8027e-03, -4.4591e-02,  5.9330e-02,\n",
      "          9.9506e-02,  1.8389e-01, -5.2210e-03,  1.2104e-01, -2.0992e-01,\n",
      "          1.0364e-01,  8.1218e-02, -1.2945e-01,  7.0753e-02, -6.4424e-02,\n",
      "          2.2057e-02,  6.6622e-02, -1.4833e-01, -2.1010e-03, -7.5547e-02,\n",
      "          7.0121e-02, -2.5609e-02,  1.4952e-02, -7.2100e-02, -4.0599e-03,\n",
      "         -4.3151e-02, -1.3836e-01,  1.3280e-02, -4.5479e-02, -7.8890e-02,\n",
      "          6.4396e-02,  1.4547e-01,  2.8917e-01,  1.0217e-02, -4.4856e-02,\n",
      "          6.3692e-02, -1.0018e-01, -9.4498e-02, -4.2564e-02,  1.0983e-01,\n",
      "          6.6306e-02, -9.7107e-02, -5.3716e-02,  1.8530e-01, -1.6873e-02,\n",
      "         -8.7613e-02, -2.1691e-01,  5.5151e-02,  6.2975e-02,  7.7647e-02,\n",
      "          2.2221e-01,  9.9447e-02,  1.1040e-01,  1.0059e-01,  3.2987e-02,\n",
      "         -5.4907e-03,  2.1692e-03, -6.3583e-02,  8.5642e-04, -5.8547e-02,\n",
      "          6.2120e-02, -7.7881e-02,  2.0595e-01,  1.3492e-01, -9.4269e-02,\n",
      "          1.8967e-02, -7.5203e-02, -2.1665e-02, -1.1011e-01,  1.6461e-02,\n",
      "         -4.8558e-02,  2.0649e-01,  7.9847e-03, -2.3328e-01,  2.0330e-01,\n",
      "          2.8555e-03, -5.3442e-02,  2.1303e-02,  5.4639e-02,  1.0519e-01,\n",
      "         -1.4363e-01, -4.1399e-03,  1.5403e-02, -1.3467e-01,  5.9428e-02,\n",
      "         -2.0949e-03,  1.0884e-02,  1.0555e-02,  1.2663e-01,  3.7979e-02,\n",
      "         -4.8289e-02,  7.5901e-02,  3.2968e-03, -3.1998e-02, -1.5196e-01,\n",
      "         -4.9915e-02,  1.5315e-01, -2.9444e-02,  1.6825e-01,  8.6259e-02,\n",
      "          5.4433e-03, -7.3166e-02,  7.6904e-02, -2.1270e-02,  3.8187e-02,\n",
      "          4.2171e-02, -1.1679e-02,  1.2124e-01, -4.1204e-02, -1.1498e-01,\n",
      "          6.3484e-02,  1.6806e-01,  4.0755e-02,  6.7181e-02, -3.2334e-02,\n",
      "          4.0177e-02,  1.0410e-01, -1.5108e-01,  8.7921e-02,  4.1867e-02,\n",
      "         -1.3683e-01, -4.5761e-02,  8.8953e-02,  8.2428e-02,  1.4882e-01,\n",
      "          1.2601e-01,  1.2327e-01, -1.6361e-01,  1.5627e-02,  2.7434e-02,\n",
      "          4.8196e-02,  6.4572e-02, -1.1700e-01,  1.2106e-01,  1.4695e-01,\n",
      "          6.3310e-02,  4.0247e-02, -7.7058e-02, -1.7589e-01, -1.0917e-01,\n",
      "         -1.4071e-01, -1.8750e-01, -1.6063e-01,  2.3421e-01,  6.3075e-02,\n",
      "          1.7036e-01, -3.3304e-01, -5.7520e-02,  7.1696e-02,  3.4390e-02,\n",
      "         -5.5049e-02,  1.1484e-02,  1.9172e-02, -1.3809e-02,  3.6680e-02,\n",
      "          1.9510e-01,  6.8688e-02,  7.7335e-03,  2.0894e-02,  1.0706e-01,\n",
      "          5.4010e-03,  6.7519e-02,  1.9506e-01, -1.4733e-01, -8.0701e-02,\n",
      "          1.5502e-01,  9.7190e-02, -9.1471e-03, -1.4426e-02, -7.5003e-02,\n",
      "          1.1836e-01,  8.5229e-03,  5.1440e-02,  5.6066e-02,  1.1982e-01,\n",
      "         -5.9159e-02,  5.0773e-02,  6.4301e-02,  9.0255e-03, -1.2637e-01,\n",
      "          1.0881e-01, -1.0931e-01, -8.4456e-02, -3.8994e-02,  1.1782e-01,\n",
      "         -2.5435e-02, -1.4933e-01, -2.2371e-01, -1.9316e-01,  3.3526e-02,\n",
      "         -8.9153e-02, -4.0061e-02,  4.3586e-02,  8.0100e-02, -2.0335e-02,\n",
      "         -1.0348e-01, -1.7139e-01, -7.1442e-02,  1.0568e-01,  1.4110e-01,\n",
      "         -1.2707e-02,  1.6082e-01,  1.2305e-01,  1.5598e-01, -5.0624e-03,\n",
      "          2.7164e-01, -3.1514e-02, -5.9700e-02,  8.5155e-02,  1.3023e-01,\n",
      "          7.6969e-02, -1.2731e-02,  1.3560e-01, -1.3151e-01,  8.7210e-03,\n",
      "          3.7587e-02,  1.1560e-01,  2.8542e-03,  1.1575e-01,  2.2441e-01,\n",
      "          9.5430e-03, -9.1490e-02,  2.0857e-01,  2.2380e-01, -1.9684e-01,\n",
      "         -6.5772e-02,  1.2329e-01,  3.5582e-02, -1.4959e-01,  1.1142e-01,\n",
      "         -1.1241e-01,  5.7173e-02, -2.0406e-02,  8.0693e-02,  9.4808e-03,\n",
      "          6.3746e-02,  1.6224e-01, -1.4862e-01, -7.3906e-03,  3.4749e-04,\n",
      "         -1.9364e-02,  2.1197e-02, -3.5937e-02, -7.2143e-02, -1.7063e-02,\n",
      "          2.0129e-01,  6.2926e-02, -4.5878e-02, -4.9531e-02, -6.6362e-02,\n",
      "          8.5832e-02,  1.3616e-02, -1.1344e-03,  3.2507e-02, -1.9842e-02,\n",
      "          1.6209e-01, -5.6135e-02,  8.3629e-02, -1.8006e-01,  1.4853e-01,\n",
      "          3.3037e-02, -1.3808e-01, -1.4855e-01,  1.1970e-01, -2.4819e-02,\n",
      "         -3.6308e-02, -2.8083e-01,  3.7267e-03,  3.9465e-02,  1.4445e-02,\n",
      "          5.2743e-04,  2.2573e-01,  1.6733e-01,  1.2205e-01, -1.1468e-02,\n",
      "          3.6791e-01,  1.6376e-01,  1.1797e-01,  1.8149e-01,  1.5394e-01,\n",
      "          1.3461e-01, -1.0793e-01, -2.0894e-01, -9.7606e-02,  9.7320e-03,\n",
      "          7.8716e-02,  3.8718e-02,  1.2491e-01, -8.3151e-02, -3.8251e-05,\n",
      "         -3.8843e-03,  1.4714e-02,  3.9129e-02, -1.5560e-01, -9.9755e-02,\n",
      "          4.5105e-02, -2.4437e-02, -7.5485e-02,  1.0427e-02,  8.9634e-02,\n",
      "         -4.7720e-02, -2.2796e-02,  5.4294e-02,  3.2581e-03,  1.8658e-01,\n",
      "          5.6548e-02,  2.0669e-01,  1.7921e-01,  2.1081e-01,  1.9678e-01,\n",
      "          1.1102e-01,  4.3734e-02,  1.8776e-01, -8.0192e-03,  1.5203e-01,\n",
      "         -1.6391e-01, -2.7525e-01, -1.6972e-01, -2.8260e-01,  1.1786e-01,\n",
      "         -1.5072e-01, -1.0460e-01, -7.1358e-02, -1.4615e-01, -5.3576e-02,\n",
      "         -1.5735e-01,  7.3550e-02, -1.0585e-01, -3.7545e-01, -1.0455e-01,\n",
      "         -1.5668e-02,  4.4475e-03, -1.1829e-02, -1.4773e-01, -4.6052e-02,\n",
      "         -5.8442e-03,  4.9576e-03,  6.2090e-02,  5.8042e-02,  3.8417e-02,\n",
      "         -9.0834e-02,  1.3565e-02,  1.0262e-01,  2.9112e-02, -8.9607e-02]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.0811], device='cuda:0')\n",
      "Train - Loss: 2.1192 Accuracy: 0.7914\n"
     ]
    }
   ],
   "source": [
    "train(my_model, check_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `validate` function mostly remains the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            output = torch.squeeze(model(x))\n",
    "\n",
    "            loss += loss_function(output, y.float()).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print('Valid - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moment of truth: can the model learn to recognize Bo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 2.0250 Accuracy: 0.8489\n",
      "Valid - Loss: 0.7413 Accuracy: 0.7000\n",
      "Epoch: 1\n",
      "Train - Loss: 0.9755 Accuracy: 0.9065\n",
      "Valid - Loss: 0.5083 Accuracy: 0.7333\n",
      "Epoch: 2\n",
      "Train - Loss: 1.1274 Accuracy: 0.8777\n",
      "Valid - Loss: 0.3325 Accuracy: 0.8667\n",
      "Epoch: 3\n",
      "Train - Loss: 1.2637 Accuracy: 0.8561\n",
      "Valid - Loss: 0.3228 Accuracy: 0.8667\n",
      "Epoch: 4\n",
      "Train - Loss: 0.8964 Accuracy: 0.9281\n",
      "Valid - Loss: 0.3583 Accuracy: 0.8333\n",
      "Epoch: 5\n",
      "Train - Loss: 0.9752 Accuracy: 0.9065\n",
      "Valid - Loss: 0.3180 Accuracy: 0.8333\n",
      "Epoch: 6\n",
      "Train - Loss: 1.4409 Accuracy: 0.8705\n",
      "Valid - Loss: 0.2020 Accuracy: 0.9333\n",
      "Epoch: 7\n",
      "Train - Loss: 0.7667 Accuracy: 0.9281\n",
      "Valid - Loss: 0.1481 Accuracy: 0.9667\n",
      "Epoch: 8\n",
      "Train - Loss: 0.8424 Accuracy: 0.9281\n",
      "Valid - Loss: 0.1274 Accuracy: 0.9667\n",
      "Epoch: 9\n",
      "Train - Loss: 0.9258 Accuracy: 0.9353\n",
      "Valid - Loss: 0.1182 Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train(my_model, check_grad=False)\n",
    "    validate(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the training and validation accuracy should be quite high. This is a pretty awesome result! We were able to train on a small dataset, but because of the knowledge transferred from the ImageNet model, it was able to achieve high accuracy and generalize well. This means it has a very good sense of Bo and pets who are not Bo.\n",
    "\n",
    "If you saw some fluctuation in the validation accuracy, that is okay too. We have a technique for improving our model in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the new layers of the model are trained, we have the option to apply a final trick to improve the model, called [fine-tuning](https://developers.google.com/machine-learning/glossary#f). To do this we unfreeze the entire model, and train it again with a very small [learning rate](https://developers.google.com/machine-learning/glossary#learning-rate). This will cause the base pre-trained layers to take very small steps and adjust slightly, improving the model by a small amount. VGG16 is a relatively large model,so the small learning rate will also prevent overfitting.\n",
    "\n",
    "Note that it is important to only do this step after the model with frozen layers has been fully trained. The untrained linear layer that we added to the model earlier was randomly initialized. This means it needed to be updated quite a lot to correctly classify the images. Through the process of [backpropagation](https://developers.google.com/machine-learning/glossary#backpropagation), large initial updates in the last layers would have caused potentially large updates in the pre-trained layers as well. These updates would have destroyed those important pre-trained features. However, now that those final layers are trained and have converged, any updates to the model as a whole will be much smaller (especially with a very small learning rate) and will not destroy the features of the earlier layers.\n",
    "\n",
    "Let's try unfreezing the pre-trained layers, and then fine tuning the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(True)\n",
    "optimizer = Adam(my_model.parameters(), lr=.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 0.9734 Accuracy: 0.9281\n",
      "Valid - Loss: 0.1128 Accuracy: 0.9667\n",
      "Epoch: 1\n",
      "Train - Loss: 0.7697 Accuracy: 0.9137\n",
      "Valid - Loss: 0.1101 Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train(my_model, check_grad=False)\n",
    "    validate(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll only train for a few `epochs`. Because VGG16 is such a large model, it can overfit when it trains for too long on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a well-trained model, it is time to create our doggy door for Bo! We can start by looking at the predictions that come from the model. We will preprocess the image in the same way we did for our last doggy door."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def show_image(image_path):\n",
    "    image = mpimg.imread(image_path)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(file_path):\n",
    "    show_image(file_path)\n",
    "    image = Image.open(file_path)\n",
    "    image = pre_trans(image).to(device)\n",
    "    image = image.unsqueeze(0)\n",
    "    output = my_model(image)\n",
    "    prediction = output.item()\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this out on a couple images to see the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction('data/presidential_doggy_door/valid/bo/bo_20.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction('data/presidential_doggy_door/valid/not_bo/121.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a negative number prediction means that it is Bo and a positive number prediction means it is something else. We can use this information to have our doggy door only let Bo in! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Bo's Doggy Door"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following code to implement Bo's doggy door:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def presidential_doggy_door(image_path):\n",
    "    pred = make_prediction(image_path)\n",
    "    if FIXME:\n",
    "        print(\"It's Bo! Let him in!\")\n",
    "    else:\n",
    "        print(\"That's not Bo! Stay out!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on the '...' below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "def presidential_doggy_door(image_path):\n",
    "    pred = make_prediction(image_path)\n",
    "    if pred < 0:\n",
    "        print(\"It's Bo! Let him in!\")\n",
    "    else:\n",
    "        print(\"That's not Bo! Stay out!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidential_doggy_door('data/presidential_doggy_door/valid/not_bo/131.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidential_doggy_door('data/presidential_doggy_door/valid/bo/bo_29.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work! With transfer learning, you have built a highly accurate model using a very small dataset. This can be an extremely powerful technique, and be the difference between a successful project and one that cannot get off the ground. We hope these techniques can help you out in similar situations in the future!\n",
    "\n",
    "There is a wealth of helpful resources for transfer learning in the [NVIDIA TAO Toolkit](https://developer.nvidia.com/tlt-getting-started)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear the Memory\n",
    "Before moving on, please execute the following cell to clear up the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the focus of this workshop has primarily been on image classification. In the next section, in service of giving you a more well-rounded introduction to deep learning, we are going to switch gears and address working with sequential data, which requires a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
